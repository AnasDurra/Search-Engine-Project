{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Setup and Imports \n",
    "\n",
    "This section initializes the environment, sets up necessary constants, and imports required libraries and modules.\n",
    "\n",
    "#### Environment Setup\n",
    "Load environment variables from the `.env` file.\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "ANTIQUE_DATASET_PATH = os.getenv('ANTIQUE_DATASET_PATH')\n",
    "RECALL_PRECISION_THRESHOLD = int(os.getenv('RECALL_PRECISION_THRESHOLD', 10))"
   ],
   "id": "2bf826011b11c83d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-28T18:45:12.670657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import  Dict\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from database.mongo_helper import MongoDBConnection\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from overrides import overrides\n",
    "import os\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from numpy import ndarray\n",
    "from common.constants import Locations\n",
    "from common.file_utilities import FileUtilities\n",
    "from database.chroma_helper import ChromaHelper\n",
    "from text_processors.base_text_processor import BaseTextProcessor\n",
    "from text_processors.antique_text_processor import AntiqueTextProcessor\n",
    "from tabulate import tabulate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ANTIQUE_DATASET_PATH = os.getenv('ANTIQUE_DATASET_PATH')\n",
    "RECALL_PRECISION_THRESHOLD = int(os.getenv('RECALL_PRECISION_THRESHOLD', 10))\n"
   ],
   "id": "5d9806df48a0302f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define Evaluation Metrics Calculators",
   "id": "196a4afe635654d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MetricCalculator(ABC):\n",
    "    @abstractmethod\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        pass\n",
    "\n",
    "class AveragePrecisionCalculator(MetricCalculator):\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        if query_id not in qrels:\n",
    "            return 0.0\n",
    "\n",
    "        relevant_docs = qrels[query_id]\n",
    "        num_retrieved_relevant_docs = 0\n",
    "        sum_precisions = 0.0\n",
    "        num_relevant_docs = 0\n",
    "\n",
    "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
    "            if doc_id in relevant_docs and relevant_docs[doc_id] > 0:\n",
    "                num_retrieved_relevant_docs += 1\n",
    "                num_relevant_docs += 1\n",
    "                precision_at_i = num_retrieved_relevant_docs / i\n",
    "                sum_precisions += precision_at_i\n",
    "\n",
    "        average_precision = 0 if num_relevant_docs == 0 else sum_precisions / num_relevant_docs\n",
    "        return average_precision\n",
    "\n",
    "class PrecisionCalculator(MetricCalculator):\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        if not retrieved_docs:\n",
    "            return 0.0\n",
    "\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        relevant_retrieved = sum(1 for doc_id in retrieved_docs[:k] if doc_id in relevant_docs)\n",
    "\n",
    "        if not relevant_retrieved:\n",
    "            return 0.0\n",
    "        return relevant_retrieved / min(len(retrieved_docs), k)\n",
    "\n",
    "class RecallCalculator(MetricCalculator):\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        relevant_retrieved = sum(1 for doc_id in retrieved_docs[:k] if doc_id in relevant_docs)\n",
    "        total_relevant = sum(relevant_docs.values())\n",
    "        return relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "class ReciprocalRankCalculator(MetricCalculator):\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "\n",
    "        if k is not None:\n",
    "            retrieved_docs = retrieved_docs[:k]\n",
    "\n",
    "        for i, doc in enumerate(retrieved_docs, start=1):\n",
    "            doc_id = doc['doc_id']\n",
    "            if doc_id in relevant_docs.keys() and relevant_docs[doc_id] > 0:\n",
    "                return 1.0 / i\n",
    "        return 0.0\n"
   ],
   "id": "9e34349102fe78b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define Evaluation Manager",
   "id": "a5b73f06a112b76f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EvaluationManager:\n",
    "    def __init__(self, metric_calculators: List[MetricCalculator], matcher):\n",
    "        self.metric_calculators = metric_calculators\n",
    "        self.matcher = matcher\n",
    "\n",
    "    def evaluate(self, queries: Dict[str, str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> Dict[str, Dict[str, float]]:\n",
    "        evaluation_results = {}\n",
    "\n",
    "        for query_id, query_text in queries.items():\n",
    "            retrieved_docs = self.matcher.match(query_text, k)\n",
    "            metrics_results = {}\n",
    "            for metric_calculator in self.metric_calculators:\n",
    "                metric_name = metric_calculator.__class__.__name__\n",
    "                if metric_name in [\"AveragePrecisionCalculator\", \"RecallCalculator\", \"PrecisionCalculator\"]:\n",
    "                    retrieved_doc_ids = [doc_info['doc_id'] for doc_info in retrieved_docs]\n",
    "                    metric_value = metric_calculator.calculate(query_id, retrieved_doc_ids, qrels, k=k)\n",
    "                else:\n",
    "                    metric_value = metric_calculator.calculate(query_id, retrieved_docs, qrels)\n",
    "                metrics_results[metric_name] = metric_value\n",
    "\n",
    "            evaluation_results[query_id] = metrics_results\n",
    "\n",
    "        return evaluation_results\n"
   ],
   "id": "60888a3505939500"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load Data",
   "id": "4f193abee2a91b30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DatasetReader:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_as_dict(self) -> dict:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_queries(self) -> dict:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_qrels(self) -> defaultdict:\n",
    "        pass\n",
    "    \n",
    "class AntiqueReader(DatasetReader):\n",
    "    @overrides\n",
    "    def load_as_dict(self) -> dict:\n",
    "        key_value_pairs = {}\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                key, value = line.strip().split('\\t')\n",
    "                key_value_pairs[key] = value\n",
    "        return key_value_pairs\n",
    "\n",
    "    @overrides\n",
    "    def read_queries(self) -> dict:\n",
    "        queries_path = os.environ.get('ANTIQUE_QUERIES_PATH')\n",
    "        queries = {}\n",
    "        with open(queries_path, 'r') as f:\n",
    "            for line in f:\n",
    "                query_id, query_text = line.strip().split('\\t')\n",
    "                queries[query_id] = query_text\n",
    "        return queries\n",
    "\n",
    "    @overrides\n",
    "    def read_qrels(self) -> defaultdict:\n",
    "        qrels_path = os.environ.get('ANTIQUE_QRELS_PATH')\n",
    "        qrels = defaultdict(dict)\n",
    "        with open(qrels_path, 'r') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                parts = re.split(r'\\s+', line.strip())\n",
    "\n",
    "                query_id, _, doc_id, relevance = parts\n",
    "                qrels[query_id][doc_id] = int(relevance)\n",
    "\n",
    "        return qrels\n",
    "    \n",
    "def load_antique_data():\n",
    "    reader = AntiqueReader(ANTIQUE_DATASET_PATH)\n",
    "    qrels = reader.read_qrels()\n",
    "    queries = reader.read_queries()\n",
    "    return qrels, queries\n",
    "\n",
    "qrels, queries = load_antique_data()"
   ],
   "id": "9b267bbe2cda9b13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define Matchers",
   "id": "1a5017919bdc6854"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class QueryMatcher:\n",
    "    def __init__(self, model_name: str):\n",
    "        matrix_path: str = Locations.generate_matrix_path(model_name)\n",
    "        self.matrix = FileUtilities.load_file(matrix_path)\n",
    "\n",
    "        model_path: str = Locations.generate_model_path(model_name)\n",
    "        self.model: TfidfVectorizer = FileUtilities.load_file(model_path)\n",
    "\n",
    "        self.threshold = float(os.environ.get('SIMILARITY_THRESHOLD', 0.5))\n",
    "\n",
    "        self.db_collection = MongoDBConnection.get_instance().get_collection(model_name)\n",
    "\n",
    "    def __vectorize_query(self, query: str):\n",
    "        return self.model.transform([query])\n",
    "\n",
    "    def match(self, query: str, n=None):\n",
    "        print(f\"Query: {query}\")\n",
    "\n",
    "        query_vector = self.__vectorize_query(query)\n",
    "\n",
    "        cos_similarities = cosine_similarity(self.matrix, query_vector)\n",
    "\n",
    "        sorted_indices = np.argsort(cos_similarities, axis=0)[::-1].flatten()\n",
    "\n",
    "        matching_docs_indices = []\n",
    "        for i in sorted_indices:\n",
    "            if cos_similarities[i].item() >= self.threshold:\n",
    "                matching_docs_indices.append(i.item() + 1)\n",
    "\n",
    "        matching_results = list(self.db_collection.find({\"index\": {\"$in\": matching_docs_indices}}))\n",
    "\n",
    "        return sorted(\n",
    "            matching_results,\n",
    "            key=lambda x: matching_docs_indices.index(x['index']),\n",
    "            reverse=False\n",
    "        )\n",
    "    \n",
    "\n",
    "class AntiqueMatcher(QueryMatcher):\n",
    "    def __init__(self):\n",
    "        super().__init__(Locations.ANTIQUE_COLLECTION_NAME)"
   ],
   "id": "24e40c57def6d18e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate Without Embedding",
   "id": "9e5b9f0b03b79561"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "matcher = AntiqueMatcher()\n",
    "\n",
    "metric_calculators = [\n",
    "    AveragePrecisionCalculator(),\n",
    "    PrecisionCalculator(),\n",
    "    RecallCalculator(),\n",
    "    ReciprocalRankCalculator()\n",
    "]\n",
    "\n",
    "evaluation_manager = EvaluationManager(metric_calculators, matcher)\n",
    "evaluation_results_no_embedding = evaluation_manager.evaluate(queries, qrels, RECALL_PRECISION_THRESHOLD)\n",
    "\n",
    "def calculate_average_metrics(evaluation_results):\n",
    "    total_map = sum(metrics_results[\"AveragePrecisionCalculator\"] for metrics_results in evaluation_results.values())\n",
    "    total_mrr = sum(metrics_results[\"ReciprocalRankCalculator\"] for metrics_results in evaluation_results.values())\n",
    "    total_queries = len(evaluation_results)\n",
    "\n",
    "    average_map = total_map / total_queries if total_queries > 0 else 0.0\n",
    "    average_mrr = total_mrr / total_queries if total_queries > 0 else 0.0\n",
    "\n",
    "    return average_map, average_mrr\n",
    "\n",
    "average_map_no_embedding, average_mrr_no_embedding = calculate_average_metrics(evaluation_results_no_embedding)\n",
    "print(f\"Average MAP without embedding: {average_map_no_embedding:.6f}\")\n",
    "print(f\"Average MRR without embedding: {average_mrr_no_embedding:.6f}\")\n",
    "\n",
    "table = []\n",
    "for query_id, metrics in evaluation_results_no_embedding.items():\n",
    "    row = [\n",
    "        query_id,\n",
    "        f\"{metrics['PrecisionCalculator']:.6f}\",\n",
    "        f\"{metrics['RecallCalculator']:.6f}\"\n",
    "    ]\n",
    "    table.append(row)\n",
    "\n",
    "print(\"\\nDetailed Results Without Embedding:\")\n",
    "print(tabulate(table, headers=[\"Query ID\", \"Precision\", \"Recall\"], tablefmt=\"pretty\"))"
   ],
   "id": "432eac22ce0317ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Improving Precision With Embedding",
   "id": "4a58b1e20dce9431"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Embedding Matcher",
   "id": "5c150d6c65d79360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BaseEmbeddingMatcher:\n",
    "\n",
    "    def __init__(self, model_name: str, text_processor: BaseTextProcessor):\n",
    "        self.vector_collection = ChromaHelper.get_instance().get_or_create_collection(model_name)\n",
    "        self.vector_size = int(os.environ.get(\"VECTOR_SIZE\", 500))\n",
    "        self.model: Word2Vec = self.__load_model(model_name)\n",
    "        self.text_processor = text_processor\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def match(self, text: str, top: int = 10):\n",
    "        processed_query: List[str] = self.text_processor.process_query(text)\n",
    "\n",
    "        query_embeddings: List = self.vectorize_query(processed_query).tolist()\n",
    "\n",
    "        result = self.vector_collection.query(\n",
    "            query_embeddings=query_embeddings,\n",
    "            n_results=top,\n",
    "        )\n",
    "\n",
    "        transformed_results = []\n",
    "        ids = result.get('ids', [[]])[0]\n",
    "        documents = result.get('documents', [[]])[0]\n",
    "        distances = result.get('distances', [[]])[0]\n",
    "\n",
    "        for doc_id, doc_content, doc_similarity in zip(ids, documents, distances):\n",
    "            transformed_results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'doc_content': doc_content,\n",
    "                'similarity': doc_similarity,\n",
    "            })\n",
    "\n",
    "        return transformed_results\n",
    "\n",
    "    def vectorize_query(self, query_words: list[str]) -> ndarray:\n",
    "\n",
    "        query_vectors = [self.model.wv[word] for word in query_words if word in self.model.wv]\n",
    "\n",
    "        if query_vectors:\n",
    "            query_vec = np.mean(query_vectors, axis=0)\n",
    "        else:\n",
    "            query_vec = np.zeros(self.vector_size)\n",
    "\n",
    "        return query_vec\n",
    "\n",
    "    @staticmethod\n",
    "    def __load_model(model_name: str):\n",
    "        return FileUtilities.load_file(\n",
    "            file_path=Locations.generate_embeddings_model_path(model_name)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class AntiqueEmbeddingMatcher(BaseEmbeddingMatcher):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name='antique',\n",
    "            text_processor=AntiqueTextProcessor()\n",
    "        )"
   ],
   "id": "30e709fa005a1a01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate With Embedding",
   "id": "3643c6091f2c5efe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "matcher = AntiqueEmbeddingMatcher()\n",
    "evaluation_manager = EvaluationManager(metric_calculators, matcher)\n",
    "evaluation_results_with_embedding = evaluation_manager.evaluate(queries, qrels, RECALL_PRECISION_THRESHOLD)\n",
    "\n",
    "average_map_with_embedding, average_mrr_with_embedding = calculate_average_metrics(evaluation_results_with_embedding)\n",
    "print(f\"Average MAP with embedding: {average_map_with_embedding:.6f}\")\n",
    "print(f\"Average MRR with embedding: {average_mrr_with_embedding:.6f}\")\n",
    "\n",
    "table = []\n",
    "for query_id, metrics in evaluation_results_with_embedding.items():\n",
    "    row = [\n",
    "        query_id,\n",
    "        f\"{metrics['PrecisionCalculator']:.6f}\",\n",
    "        f\"{metrics['RecallCalculator']:.6f}\"\n",
    "    ]\n",
    "    table.append(row)\n",
    "\n",
    "print(\"\\nDetailed Results With Embedding:\")\n",
    "print(tabulate(table, headers=[\"Query ID\", \"Precision\", \"Recall\"], tablefmt=\"pretty\"))"
   ],
   "id": "63af15ac701754a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
