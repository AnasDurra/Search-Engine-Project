{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf826011b11c83d",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports \n",
    "\n",
    "This section initializes the environment, sets up necessary constants, and imports required libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "id": "5d9806df48a0302f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:30:44.958283Z",
     "start_time": "2024-06-01T13:30:43.131292Z"
    }
   },
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from numpy import ndarray\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional\n",
    "from typing import  Dict\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from database.mongo_helper import MongoDBConnection\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from common.constants import Locations\n",
    "from common.file_utilities import FileUtilities\n",
    "from database.chroma_helper import ChromaHelper\n",
    "from text_processors.antique_text_processor import AntiqueTextProcessor\n",
    "from tabulate import tabulate\n",
    "from overrides import overrides\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, PorterStemmer, ne_chunk\n",
    "from nltk.corpus import wordnet\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "WIKIPEDIA_DATASET_PATH = os.getenv('WIKIPEDIA_DATASET_PATH')\n",
    "RECALL_PRECISION_THRESHOLD = int(os.getenv('RECALL_PRECISION_THRESHOLD', 10))\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "196a4afe635654d5",
   "metadata": {},
   "source": [
    "### 2. Define Evaluation Metrics Calculators\n",
    "\n",
    "in this section we are creating the routines used to calculate evaluations later for MAP, MRR, Recall@10, precision@10"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e34349102fe78b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:30:44.976242Z",
     "start_time": "2024-06-01T13:30:44.961291Z"
    }
   },
   "source": [
    "class MetricCalculator(ABC):\n",
    "    @abstractmethod\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        pass\n",
    "\n",
    "class AveragePrecisionCalculator(MetricCalculator):\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        if query_id not in qrels:\n",
    "            return 0.0\n",
    "\n",
    "        relevant_docs = qrels[query_id]\n",
    "        num_retrieved_relevant_docs = 0\n",
    "        sum_precisions = 0.0\n",
    "        num_relevant_docs = 0\n",
    "\n",
    "        for i, doc_id in enumerate(retrieved_docs, start=1):\n",
    "            if doc_id in relevant_docs and relevant_docs[doc_id] > 0:\n",
    "                num_retrieved_relevant_docs += 1\n",
    "                num_relevant_docs += 1\n",
    "                precision_at_i = num_retrieved_relevant_docs / i\n",
    "                sum_precisions += precision_at_i\n",
    "\n",
    "        average_precision = 0 if num_relevant_docs == 0 else sum_precisions / num_relevant_docs\n",
    "        return average_precision\n",
    "\n",
    "class PrecisionCalculator(MetricCalculator):\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        if not retrieved_docs:\n",
    "            return 0.0\n",
    "\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        relevant_retrieved = sum(1 for doc_id in retrieved_docs[:k] if doc_id in relevant_docs)\n",
    "\n",
    "        if not relevant_retrieved:\n",
    "            return 0.0\n",
    "        return relevant_retrieved / min(len(retrieved_docs), k)\n",
    "\n",
    "class RecallCalculator(MetricCalculator):\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "        relevant_retrieved = sum(1 for doc_id in retrieved_docs[:k] if doc_id in relevant_docs)\n",
    "        total_relevant = sum(relevant_docs.values())\n",
    "        return relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "\n",
    "class ReciprocalRankCalculator(MetricCalculator):\n",
    "    def calculate(self, query_id: str, retrieved_docs: List[str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> float:\n",
    "        relevant_docs = qrels.get(query_id, {})\n",
    "\n",
    "        if k is not None:\n",
    "            retrieved_docs = retrieved_docs[:k]\n",
    "\n",
    "        for i, doc in enumerate(retrieved_docs, start=1):\n",
    "            doc_id = doc['doc_id']\n",
    "            if doc_id in relevant_docs.keys() and relevant_docs[doc_id] > 0:\n",
    "                return 1.0 / i\n",
    "        return 0.0\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "a5b73f06a112b76f",
   "metadata": {},
   "source": [
    "### 3. Define Evaluation Manager\n",
    "EvaluationManager is an extra class responsible for evaluating the qrels & queries passed for a set of (matcher, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "id": "60888a3505939500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:30:44.988475Z",
     "start_time": "2024-06-01T13:30:44.979249Z"
    }
   },
   "source": [
    "class EvaluationManager:\n",
    "    def __init__(self, metric_calculators: List[MetricCalculator], matcher):\n",
    "        self.metric_calculators = metric_calculators\n",
    "        self.matcher = matcher\n",
    "\n",
    "    def evaluate(self, queries: Dict[str, str], qrels: Dict[str, Dict[str, int]], k: Optional[int] = None) -> Dict[str, Dict[str, float]]:\n",
    "        evaluation_results = {}\n",
    "\n",
    "        for query_id, query_text in queries.items():\n",
    "            retrieved_docs = self.matcher.match(query_text)[:k]\n",
    "            metrics_results = {}\n",
    "            for metric_calculator in self.metric_calculators:\n",
    "                metric_name = metric_calculator.__class__.__name__\n",
    "                if metric_name in [\"AveragePrecisionCalculator\", \"RecallCalculator\", \"PrecisionCalculator\"]:\n",
    "                    retrieved_doc_ids = [doc_info['doc_id'] for doc_info in retrieved_docs]\n",
    "                    metric_value = metric_calculator.calculate(query_id, retrieved_doc_ids, qrels, k=k)\n",
    "                else:\n",
    "                    metric_value = metric_calculator.calculate(query_id, retrieved_docs, qrels)\n",
    "                metrics_results[metric_name] = metric_value\n",
    "\n",
    "            evaluation_results[query_id] = metrics_results\n",
    "\n",
    "        return evaluation_results\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "4f193abee2a91b30",
   "metadata": {},
   "source": [
    "### 4. Load Data\n",
    "in this section we are loading the trained model `load_as_dict`, qrels, and queries."
   ]
  },
  {
   "cell_type": "code",
   "id": "9b267bbe2cda9b13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:30:45.013511Z",
     "start_time": "2024-06-01T13:30:44.990485Z"
    }
   },
   "source": [
    "class DatasetReader:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_as_dict(self) -> dict:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_queries(self) -> dict:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_qrels(self) -> defaultdict:\n",
    "        pass\n",
    "    \n",
    "class WikipediaReader(DatasetReader, ABC):\n",
    "\n",
    "    @overrides\n",
    "    def load_as_dict(self) -> dict:\n",
    "        key_value_pairs = {}\n",
    "\n",
    "        df = pd.read_csv(self.file_path)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            key = str(row['id_right'])\n",
    "            value = str(row['text_right'])\n",
    "\n",
    "            key_value_pairs[key] = value\n",
    "\n",
    "        return key_value_pairs\n",
    "\n",
    "    @overrides\n",
    "    def read_queries(self) -> dict:\n",
    "        queries_path = os.environ.get('WIKIPEDIA_QUERIES_PATH', '../data/wikipedia/queries.csv')\n",
    "        queries = {}\n",
    "        with open(queries_path, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader)  # Skip header\n",
    "            for row in reader:\n",
    "                queries[row[0]] = row[1]\n",
    "        return queries\n",
    "\n",
    "    @overrides\n",
    "    def read_qrels(self) -> defaultdict:\n",
    "        qrels_path = os.environ.get('WIKIPEDIA_QRELS_PATH', '../data/wikipedia/qrels')\n",
    "        qrels = defaultdict(dict)\n",
    "\n",
    "        with open(qrels_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                query_id, _, doc_id, relevance = parts\n",
    "                qrels[query_id][doc_id] = int(relevance)\n",
    "\n",
    "        return qrels\n",
    "    \n",
    "def load_wikipedia_data():\n",
    "    reader = WikipediaReader(WIKIPEDIA_DATASET_PATH)\n",
    "    qrels = reader.read_qrels()\n",
    "    queries = reader.read_queries()\n",
    "    return qrels, queries\n",
    "\n",
    "qrels, queries = load_wikipedia_data()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Text Processing\n",
    "text processing steps used for training the model and preparing queries"
   ],
   "id": "1d11aedf72f6f43f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:30:45.041777Z",
     "start_time": "2024-06-01T13:30:45.016520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def get_wordnet_pos(tag_parameter):\n",
    "    tag = tag_parameter[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "class BaseTextProcessor:\n",
    "    def __init__(self) -> None:\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.spell_checker = SpellChecker(distance=4)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.tokenizer = word_tokenize\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.pos_tagger = pos_tag\n",
    "\n",
    "    def process(self, text) -> List[str]:\n",
    "        pass\n",
    "\n",
    "    def process_query(self, query: str) -> List[str]:\n",
    "        pass\n",
    "\n",
    "    def _word_tokenizer(self, text: str) -> List[str]:\n",
    "        tokens = self.tokenizer(text)\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def _lowercase_tokens(tokens: List[str]) -> List[str]:\n",
    "        return [str(np.char.lower(token)) for token in tokens]\n",
    "\n",
    "    def _filter_stop_words(self, tokens: List[str]) -> List[str]:\n",
    "        return [token for token in tokens if token not in self.stop_words and len(token) > 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_registered_markers(tokens: List[str]) -> List[str]:\n",
    "        return [re.sub(r'\\u00AE', '', token) for token in tokens if token is not None]\n",
    "\n",
    "    @staticmethod\n",
    "    def _strip_punctuation(tokens: List[str]) -> List[str]:\n",
    "        return [\n",
    "            token.translate(str.maketrans('', '', string.punctuation))\n",
    "            for token in tokens if token is not None\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _eliminate_whitespaces(tokens: List[str]) -> List[str]:\n",
    "        return [re.sub(r'_', ' ', token) for token in tokens if token is not None]\n",
    "\n",
    "    @staticmethod\n",
    "    def _remove_apostrophes(tokens: List[str]) -> List[str]:\n",
    "        return [str(np.char.replace(token, \"'\", \" \")) for token in tokens if token is not None]\n",
    "\n",
    "    def _apply_stemming(self, tokens: List[str]) -> List[str]:\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_abbreviations(tokens: List[str]) -> List[str]:\n",
    "        resolved_terms = {}\n",
    "        for token in tokens:\n",
    "\n",
    "            if len(token) >= 2:\n",
    "                synsets = wordnet.synsets(token)\n",
    "                if synsets:\n",
    "                    resolved_term = synsets[0].lemmas()[0].name()\n",
    "                    resolved_terms[token] = resolved_term\n",
    "\n",
    "        for abbreviation, resolved_term in resolved_terms.items():\n",
    "            for i in range(len(tokens)):\n",
    "                if tokens[i] == abbreviation:\n",
    "                    tokens[i] = resolved_term\n",
    "                    break\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _lemmatize_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        lemmatizer = self.lemmatizer\n",
    "        pos_tags = self.pos_tagger(tokens)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def _spell_check(self, tokens: List[str]) -> List[str]:\n",
    "        return [self.spell_checker.correction(word) if isinstance(word, str) else word for word in tokens if\n",
    "                word is not None]\n",
    "    @staticmethod\n",
    "    def get_tokens_as_string(tokens: List[str]) -> str:\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def _apply_named_entity_recognition(self, tokens: List[str]) -> List[str]:\n",
    "        tagged_tokens = self.pos_tagger(tokens)\n",
    "        ner_tags = ne_chunk(tagged_tokens)\n",
    "        entities = []\n",
    "\n",
    "        def extract_entity_text(chunk):\n",
    "            if isinstance(chunk, nltk.tree.Tree):\n",
    "                return ' '.join([extract_entity_text(c) for c in chunk])\n",
    "            else:\n",
    "                return chunk[0]\n",
    "\n",
    "        for chunk in ner_tags:\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'NE':\n",
    "                entity = extract_entity_text(chunk.leaves())\n",
    "                entities.append(str(entity))\n",
    "            else:\n",
    "                entity = chunk[0] if isinstance(chunk, nltk.tree.Tree) else chunk[0][0]\n",
    "                entities.append(str(entity))\n",
    "\n",
    "        return entities\n",
    "\n",
    "class WikipediaTextProcessor(BaseTextProcessor):\n",
    "    @overrides\n",
    "    def process(self, text) -> List[str]:\n",
    "        tokens = self._word_tokenizer(text)\n",
    "        tokens = self._lowercase_tokens(tokens)\n",
    "        tokens = self._strip_punctuation(tokens)\n",
    "        tokens = self._remove_apostrophes(tokens)\n",
    "        tokens = self._filter_stop_words(tokens)\n",
    "        tokens = self._remove_registered_markers(tokens)\n",
    "        tokens = self._lemmatize_tokens(tokens)\n",
    "        tokens = self._normalize_abbreviations(tokens)\n",
    "        #tokens = self._spell_check(tokens)\n",
    "        tokens = self._lowercase_tokens(tokens)\n",
    "        tokens = self._eliminate_whitespaces(tokens)\n",
    "        return tokens\n"
   ],
   "id": "a63a1e4c4ac754c8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "1a5017919bdc6854",
   "metadata": {},
   "source": [
    "### 6. Matching\n",
    "matcher classes are used to match a query to a set of documents, it calculates the similarity between query and document using `cos_similarity`"
   ]
  },
  {
   "cell_type": "code",
   "id": "24e40c57def6d18e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:30:45.054295Z",
     "start_time": "2024-06-01T13:30:45.043784Z"
    }
   },
   "source": [
    "class QueryMatcher:\n",
    "    def __init__(self, model_name: str):\n",
    "        matrix_path: str = Locations.generate_matrix_path(model_name)\n",
    "        self.matrix = FileUtilities.load_file(matrix_path)\n",
    "\n",
    "        model_path: str = Locations.generate_model_path(model_name)\n",
    "        self.model: TfidfVectorizer = FileUtilities.load_file(model_path)\n",
    "\n",
    "        self.threshold = float(os.environ.get('SIMILARITY_THRESHOLD', 0.1))\n",
    "\n",
    "        self.db_collection = MongoDBConnection.get_instance().get_collection(model_name)\n",
    "\n",
    "    def __vectorize_query(self, query: str):\n",
    "        return self.model.transform([query])\n",
    "\n",
    "    def match(self, query: str):\n",
    "        query_vector = self.__vectorize_query(query)\n",
    "\n",
    "        cos_similarities = cosine_similarity(self.matrix, query_vector)\n",
    "\n",
    "        sorted_indices = np.argsort(cos_similarities, axis=0)[::-1].flatten()\n",
    "\n",
    "        matching_docs_indices = []\n",
    "        for i in sorted_indices:\n",
    "            if cos_similarities[i].item() >= self.threshold:\n",
    "                matching_docs_indices.append(i.item() + 1)\n",
    "\n",
    "        matching_results = list(self.db_collection.find({\"index\": {\"$in\": matching_docs_indices}}))\n",
    "\n",
    "        return sorted(\n",
    "            matching_results,\n",
    "            key=lambda x: matching_docs_indices.index(x['index']),\n",
    "            reverse=False\n",
    "        )\n",
    "    \n",
    "\n",
    "class WikipediaMatcher(QueryMatcher):\n",
    "    def __init__(self):\n",
    "        super().__init__(Locations.WIKIPEDIA_COLLECTION_NAME)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "9e5b9f0b03b79561",
   "metadata": {},
   "source": "### 7. Results (Without Embedding)"
  },
  {
   "cell_type": "code",
   "id": "432eac22ce0317ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:32:54.924780Z",
     "start_time": "2024-06-01T13:30:45.056302Z"
    }
   },
   "source": [
    "matcher = WikipediaMatcher()\n",
    "\n",
    "metric_calculators = [\n",
    "    AveragePrecisionCalculator(),\n",
    "    PrecisionCalculator(),\n",
    "    RecallCalculator(),\n",
    "    ReciprocalRankCalculator()\n",
    "]\n",
    "\n",
    "evaluation_manager = EvaluationManager(metric_calculators, matcher)\n",
    "evaluation_results_no_embedding = evaluation_manager.evaluate(queries, qrels, RECALL_PRECISION_THRESHOLD)\n",
    "\n",
    "def calculate_average_metrics(evaluation_results):\n",
    "    total_map = sum(metrics_results[\"AveragePrecisionCalculator\"] for metrics_results in evaluation_results.values())\n",
    "    total_mrr = sum(metrics_results[\"ReciprocalRankCalculator\"] for metrics_results in evaluation_results.values())\n",
    "    total_queries = len(evaluation_results)\n",
    "\n",
    "    average_map = total_map / total_queries if total_queries > 0 else 0.0\n",
    "    average_mrr = total_mrr / total_queries if total_queries > 0 else 0.0\n",
    "\n",
    "    return average_map, average_mrr\n",
    "\n",
    "average_map_no_embedding, average_mrr_no_embedding = calculate_average_metrics(evaluation_results_no_embedding)\n",
    "print(f\"MAP without embedding: {average_map_no_embedding}\")\n",
    "print(f\"MRR without embedding: {average_mrr_no_embedding}\")\n",
    "\n",
    "table = []\n",
    "for query_id, metrics in evaluation_results_no_embedding.items():\n",
    "    row = [\n",
    "        query_id,\n",
    "        f\"{metrics['PrecisionCalculator']}\",\n",
    "        f\"{metrics['RecallCalculator']:.6f}\"\n",
    "    ]\n",
    "    table.append(row)\n",
    "\n",
    "print(\"\\nDetailed Results Without Embedding:\")\n",
    "print(tabulate(table, headers=[\"Query ID\", f'Precision@{RECALL_PRECISION_THRESHOLD}', f'Recall@{RECALL_PRECISION_THRESHOLD}'], tablefmt=\"pretty\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP without embedding: 0.5479969576719579\n",
      "MRR without embedding: 0.6204920634920635\n",
      "\n",
      "Detailed Results Without Embedding:\n",
      "+----------+--------------+-----------+\n",
      "| Query ID | Precision@10 | Recall@10 |\n",
      "+----------+--------------+-----------+\n",
      "|  158491  |   0.000000   | 0.000000  |\n",
      "|   5728   |   0.100000   | 0.062500  |\n",
      "|  13554   |   0.200000   | 0.250000  |\n",
      "|  32674   |   0.700000   | 0.875000  |\n",
      "|  406391  |   0.300000   | 0.333333  |\n",
      "|   5115   |   0.100000   | 0.111111  |\n",
      "|  15469   |   0.200000   | 0.080000  |\n",
      "|  62953   |   0.200000   | 0.285714  |\n",
      "|  152444  |   0.100000   | 0.100000  |\n",
      "|  104086  |   0.600000   | 0.545455  |\n",
      "|  145194  |   0.100000   | 0.142857  |\n",
      "|  73752   |   0.200000   | 0.285714  |\n",
      "| 1368508  |   0.200000   | 0.047619  |\n",
      "|  11534   |   0.400000   | 0.190476  |\n",
      "|  83078   |   0.000000   | 0.000000  |\n",
      "|  25174   |   0.200000   | 0.025641  |\n",
      "|  265104  |   0.500000   | 0.294118  |\n",
      "|  139082  |   0.100000   | 0.125000  |\n",
      "|  37743   |   0.200000   | 0.285714  |\n",
      "|  207224  |   0.200000   | 0.250000  |\n",
      "| 1313611  |   0.100000   | 0.125000  |\n",
      "|  641464  |   0.100000   | 0.083333  |\n",
      "|  75295   |   0.500000   | 0.555556  |\n",
      "|   2591   |   0.100000   | 0.021277  |\n",
      "|  107590  |   0.100000   | 0.125000  |\n",
      "|  362197  |   0.200000   | 0.285714  |\n",
      "|  13540   |   0.400000   | 0.001449  |\n",
      "|  25406   |   0.200000   | 0.125000  |\n",
      "|  128282  |   0.200000   | 0.153846  |\n",
      "|  267973  |   0.500000   | 0.555556  |\n",
      "|  116217  |   0.100000   | 0.083333  |\n",
      "|  16130   |   0.200000   | 0.095238  |\n",
      "|  471746  |   0.100000   | 0.111111  |\n",
      "|  113000  |   0.400000   | 0.571429  |\n",
      "|  87860   |   0.500000   | 0.027174  |\n",
      "|  25983   |   0.300000   | 0.250000  |\n",
      "|  11919   |   0.300000   | 0.111111  |\n",
      "|  22343   |   0.300000   | 0.375000  |\n",
      "|  81083   |   0.200000   | 0.250000  |\n",
      "|  172877  |   0.300000   | 0.300000  |\n",
      "|   7169   |   0.200000   | 0.250000  |\n",
      "|  13690   |   0.200000   | 0.285714  |\n",
      "|   8140   |   0.400000   | 0.444444  |\n",
      "|  206019  |   0.100000   | 0.111111  |\n",
      "|  104453  |   0.100000   | 0.142857  |\n",
      "|  73673   |   0.100000   | 0.111111  |\n",
      "|  996687  |   0.200000   | 0.285714  |\n",
      "|  121384  |   0.500000   | 0.555556  |\n",
      "|  87686   |   0.400000   | 0.500000  |\n",
      "|  410859  |   0.200000   | 0.285714  |\n",
      "|  108909  |   0.300000   | 0.250000  |\n",
      "|  92044   |   0.100000   | 0.031250  |\n",
      "|  371437  |   0.100000   | 0.066667  |\n",
      "|  151303  |   0.300000   | 0.096774  |\n",
      "|  101626  |   0.300000   | 0.157895  |\n",
      "|  79032   |   0.000000   | 0.000000  |\n",
      "|  17656   |   0.200000   | 0.166667  |\n",
      "|  15514   |   0.000000   | 0.000000  |\n",
      "| 1254734  |   0.300000   | 0.272727  |\n",
      "| 1238278  |   0.400000   | 0.173913  |\n",
      "|  21645   |   0.100000   | 0.058824  |\n",
      "| 1280405  |   0.000000   | 0.000000  |\n",
      "|  32559   |   0.000000   | 0.000000  |\n",
      "|  34939   |   0.100000   | 0.142857  |\n",
      "|  174151  |   0.100000   | 0.090909  |\n",
      "|   4900   |   0.100000   | 0.041667  |\n",
      "|  10166   |   0.200000   | 0.105263  |\n",
      "|  25726   |   0.300000   | 0.042857  |\n",
      "|   720    |   0.200000   | 0.080000  |\n",
      "|  200237  |   0.000000   | 0.000000  |\n",
      "|  14410   |   0.600000   | 0.206897  |\n",
      "|  23920   |   0.200000   | 0.095238  |\n",
      "| 1897191  |   0.000000   | 0.000000  |\n",
      "|  196387  |   0.100000   | 0.142857  |\n",
      "|  25130   |   0.400000   | 0.571429  |\n",
      "|  28232   |   0.200000   | 0.250000  |\n",
      "|  16952   |   0.600000   | 0.545455  |\n",
      "|  28485   |   0.000000   | 0.000000  |\n",
      "|  113827  |   0.100000   | 0.111111  |\n",
      "|  32219   |   0.200000   | 0.222222  |\n",
      "|   6984   |   0.100000   | 0.076923  |\n",
      "|  124119  |   0.300000   | 0.111111  |\n",
      "|  34875   |   0.100000   | 0.090909  |\n",
      "|   7911   |   0.000000   | 0.000000  |\n",
      "|   8630   |   0.100000   | 0.142857  |\n",
      "|  422206  |   0.300000   | 0.375000  |\n",
      "|  81416   |   0.400000   | 0.181818  |\n",
      "|  28106   |   0.200000   | 0.166667  |\n",
      "|  738573  |   0.000000   | 0.000000  |\n",
      "|  125791  |   0.100000   | 0.125000  |\n",
      "|  294384  |   0.300000   | 0.375000  |\n",
      "|  470620  |   0.100000   | 0.142857  |\n",
      "|  237831  |   0.200000   | 0.285714  |\n",
      "|   2161   |   0.400000   | 0.071429  |\n",
      "|  21145   |   0.200000   | 0.028169  |\n",
      "|  679227  |   0.200000   | 0.090909  |\n",
      "| 2136797  |   0.200000   | 0.285714  |\n",
      "|   5622   |   0.000000   | 0.000000  |\n",
      "| 1313598  |   0.200000   | 0.181818  |\n",
      "|  712704  |   0.200000   | 0.058824  |\n",
      "+----------+--------------+-----------+\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "4a58b1e20dce9431",
   "metadata": {},
   "source": [
    "# 8. Improving Results With Embedding\n",
    "new matcher class for embedding models, the matcher class will load embedding model on initialization"
   ]
  },
  {
   "cell_type": "code",
   "id": "30e709fa005a1a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:32:54.961011Z",
     "start_time": "2024-06-01T13:32:54.934792Z"
    }
   },
   "source": [
    "class BaseEmbeddingMatcher:\n",
    "\n",
    "    def __init__(self, model_name: str, text_processor: BaseTextProcessor):\n",
    "        self.vector_collection = ChromaHelper.get_instance().get_or_create_collection(model_name)\n",
    "        self.vector_size = int(os.environ.get(\"VECTOR_SIZE\", 500))\n",
    "        self.model: Word2Vec = self.__load_model(model_name)\n",
    "        self.text_processor = text_processor\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def match(self, text: str, top: int = 5000):\n",
    "        processed_query: List[str] = self.text_processor.process(text)\n",
    "        query_embeddings: List = self.vectorize_query(processed_query).tolist()\n",
    "\n",
    "        result = self.vector_collection.query(\n",
    "            query_embeddings=query_embeddings,\n",
    "            n_results=top,\n",
    "        )\n",
    "\n",
    "        transformed_results = []\n",
    "        ids = result.get('ids', [[]])[0]\n",
    "        documents = result.get('documents', [[]])[0]\n",
    "        distances = result.get('distances', [[]])[0]\n",
    "\n",
    "        for doc_id, doc_content, doc_similarity in zip(ids, documents, distances):\n",
    "            transformed_results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'doc_content': doc_content,\n",
    "                'similarity': doc_similarity,\n",
    "            })\n",
    "\n",
    "        return transformed_results\n",
    "\n",
    "    def vectorize_query(self, query_words: list[str]) -> ndarray:\n",
    "\n",
    "        query_vectors = [self.model.wv[word] for word in query_words if word in self.model.wv ]\n",
    "\n",
    "        if query_vectors:\n",
    "            query_vec = np.mean(query_vectors, axis=0)\n",
    "        else:\n",
    "            query_vec = np.zeros(self.vector_size)\n",
    "\n",
    "        return query_vec\n",
    "\n",
    "    @staticmethod\n",
    "    def __load_model(model_name: str):\n",
    "        return FileUtilities.load_file(\n",
    "            file_path=Locations.generate_embeddings_model_path(model_name)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class WikipediaEmbeddingMatcher(BaseEmbeddingMatcher):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model_name='wikipedia',\n",
    "            text_processor=WikipediaTextProcessor(),\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "3643c6091f2c5efe",
   "metadata": {},
   "source": [
    "### 9. Results (With Embedding)\n",
    "- ~?%+ MAP \n",
    "- ~?%+ MRR"
   ]
  },
  {
   "cell_type": "code",
   "id": "63af15ac701754a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:32:56.034866Z",
     "start_time": "2024-06-01T13:32:54.963018Z"
    }
   },
   "source": [
    "\n",
    "matcher = WikipediaEmbeddingMatcher()\n",
    "evaluation_manager = EvaluationManager(metric_calculators, matcher)\n",
    "evaluation_results_with_embedding = evaluation_manager.evaluate(queries, qrels, RECALL_PRECISION_THRESHOLD)\n",
    "\n",
    "average_map_with_embedding, average_mrr_with_embedding = calculate_average_metrics(evaluation_results_with_embedding)\n",
    "print(f\"MAP with embedding: {average_map_with_embedding:.6f}\")\n",
    "print(f\"MRR with embedding: {average_mrr_with_embedding:.6f}\")\n",
    "\n",
    "table = []\n",
    "for query_id, metrics in evaluation_results_with_embedding.items():\n",
    "    row = [\n",
    "        query_id,\n",
    "        f\"{metrics['PrecisionCalculator']}\",\n",
    "        f\"{metrics['RecallCalculator']:.6f}\"\n",
    "    ]\n",
    "    table.append(row)\n",
    "\n",
    "print(\"\\nDetailed Results With Embedding:\")\n",
    "print(tabulate(table, headers=[\"Query ID\", f'Precision@{RECALL_PRECISION_THRESHOLD}', f'Recall@{RECALL_PRECISION_THRESHOLD}'], tablefmt=\"pretty\"))"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\anasr\\\\Desktop\\\\search-engine\\\\common\\\\..\\\\engines\\\\wikipedia/embeddings_model\\\\wikipedia_embedding_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m matcher \u001B[38;5;241m=\u001B[39m WikipediaEmbeddingMatcher()\n\u001B[0;32m      2\u001B[0m evaluation_manager \u001B[38;5;241m=\u001B[39m EvaluationManager(metric_calculators, matcher)\n\u001B[0;32m      3\u001B[0m evaluation_results_with_embedding \u001B[38;5;241m=\u001B[39m evaluation_manager\u001B[38;5;241m.\u001B[39mevaluate(queries, qrels, RECALL_PRECISION_THRESHOLD)\n",
      "Cell \u001B[1;32mIn[12], line 55\u001B[0m, in \u001B[0;36mWikipediaEmbeddingMatcher.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 55\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m     56\u001B[0m         model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwikipedia\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     57\u001B[0m         text_processor\u001B[38;5;241m=\u001B[39mWikipediaTextProcessor(),\n\u001B[0;32m     58\u001B[0m     )\n",
      "Cell \u001B[1;32mIn[12], line 6\u001B[0m, in \u001B[0;36mBaseEmbeddingMatcher.__init__\u001B[1;34m(self, model_name, text_processor)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvector_collection \u001B[38;5;241m=\u001B[39m ChromaHelper\u001B[38;5;241m.\u001B[39mget_instance()\u001B[38;5;241m.\u001B[39mget_or_create_collection(model_name)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvector_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(os\u001B[38;5;241m.\u001B[39menviron\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVECTOR_SIZE\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m500\u001B[39m))\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel: Word2Vec \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__load_model(model_name)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_processor \u001B[38;5;241m=\u001B[39m text_processor\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name \u001B[38;5;241m=\u001B[39m model_name\n",
      "Cell \u001B[1;32mIn[12], line 46\u001B[0m, in \u001B[0;36mBaseEmbeddingMatcher.__load_model\u001B[1;34m(model_name)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__load_model\u001B[39m(model_name: \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m---> 46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m FileUtilities\u001B[38;5;241m.\u001B[39mload_file(\n\u001B[0;32m     47\u001B[0m         file_path\u001B[38;5;241m=\u001B[39mLocations\u001B[38;5;241m.\u001B[39mgenerate_embeddings_model_path(model_name)\n\u001B[0;32m     48\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\search-engine\\common\\file_utilities.py:15\u001B[0m, in \u001B[0;36mFileUtilities.load_file\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_file\u001B[39m(file_path):\n\u001B[1;32m---> 15\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m     16\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m joblib\u001B[38;5;241m.\u001B[39mload(f)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\anasr\\\\Desktop\\\\search-engine\\\\common\\\\..\\\\engines\\\\wikipedia/embeddings_model\\\\wikipedia_embedding_model.pkl'"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
